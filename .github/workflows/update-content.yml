name: Hourly Content Aggregation

on:
  schedule:
    - cron: '0 * * * *'   # æ¯å°æ—¶ï¼ˆUTCï¼‰
  workflow_dispatch:

concurrency:
  group: aggregator-${{ github.repository }}
  cancel-in-progress: true

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      # å›ºå®š Python ç‰ˆæœ¬ï¼Œä¿è¯ wheel å‘½ä¸­ä¸ä¾èµ–ç¨³å®š
      PY_VERSION: "3.12"
      # NLTK æ•°æ®ç¼“å­˜ç›®å½•ï¼ˆå¯è¢« actions/cache æŒä¹…åŒ–ï¼‰
      NLTK_DATA: ${{ runner.temp }}/nltk_data
      # â€”â€” å¯é€‰æ€§èƒ½å¼€å…³ï¼ˆå¯¹åº” aggregator.py å·²æ”¯æŒï¼‰â€”â€”
      # 1=æŠ“å…¨æ–‡, 0=åªç”¨RSSæ‘˜è¦
      ENABLE_FULLTEXT: "1"
      # æ¯æ¬¡æœ€å¤šæŠ“å¤šå°‘æ¡å…¨æ–‡ï¼›0=ä¸é™åˆ¶ï¼ˆä¸æ—§è¡Œä¸ºä¸€è‡´ï¼‰
      MAX_FULLTEXT_PER_RUN: "40"
      # æ‰“å°é˜¶æ®µè€—æ—¶æ—¥å¿—ï¼š1=å¼€, 0=å…³
      ENABLE_TIMING_LOGS: "1"
      # å¹¶å‘æ‘˜è¦æ•°é‡ï¼ˆå¦‚éœ€æ›´å¿«/æ›´çœé…é¢å¯è°ƒï¼‰
      SUMMARIZE_CONCURRENCY: "4"

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PY_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'

      # âœ… ç¼“å­˜ï¼šæŠ“å…¨æ–‡/æ‘˜è¦çš„æœ¬åœ°æ•°æ®åº“ + NLTK è¯åº“
      - name: Restore aggregator & NLTK cache
        uses: actions/cache@v4
        with:
          path: |
            .cache/aggregator
            summary_cache.json
            ${{ env.NLTK_DATA }}
          key: agg-nlp-${{ runner.os }}-${{ env.PY_VERSION }}-${{ hashFiles('feeds.json') }}
          restore-keys: |
            agg-nlp-${{ runner.os }}-${{ env.PY_VERSION }}-

      - name: Install deps
        run: |
          pip install -r requirements.txt

      - name: Ensure NLTK punkt (cached)
        shell: bash
        run: |
          python - << 'PY'
          import os, nltk
          nltk.data.path.insert(0, os.environ.get("NLTK_DATA",""))
          try:
              nltk.data.find("tokenizers/punkt")
              print("[nltk] punkt found in cache")
          except LookupError:
              print("[nltk] downloading punkt ...")
              nltk.download("punkt", download_dir=os.environ.get("NLTK_DATA",""))
          PY

      - name: Run aggregator
        # å¤±è´¥ä¸ç»§ç»­ç”Ÿæˆç«™ç‚¹ï¼Œä»¥å…å‘å¸ƒç©ºé¡µï¼›ä½†ä¼šæ˜¾ç¤ºå®Œæ•´æ—¥å¿—
        run: |
          set -e
          python scripts/aggregator.py

      - name: Generate sitemap
        run: |
          python scripts/generate_sitemap.py

      # âœ… åªæäº¤äº§ç‰©ï¼›ç¼“å­˜ç”± actions/cache æŒä¹…åŒ–
      - name: Commit & push (safe retry)
        run: |
          set -e
          git config --global user.name "github-actions"
          git config --global user.email "actions@github.com"
          git add posts/ sitemap.xml
          git commit -m "ğŸ¤– Auto-update content and sitemap" || { echo "Nothing to commit"; exit 0; }
          git push origin HEAD:main || {
            echo "Push failed; refetch + rebase then retry..."
            git fetch origin main
            git pull --rebase origin main
            git push origin HEAD:main
          }
