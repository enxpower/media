name: Hourly Content Aggregation

on:
  schedule:
    - cron: '0 * * * *'   # 每小时（UTC）
  workflow_dispatch:

concurrency:
  group: aggregator-${{ github.repository }}
  cancel-in-progress: true

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      # 固定 Python 版本，保证 wheel 命中与依赖稳定
      PY_VERSION: "3.12"
      # NLTK 数据缓存目录（可被 actions/cache 持久化）
      NLTK_DATA: ${{ runner.temp }}/nltk_data
      # —— 可选性能开关（对应 aggregator.py 已支持）——
      # 1=抓全文, 0=只用RSS摘要
      ENABLE_FULLTEXT: "1"
      # 每次最多抓多少条全文；0=不限制（与旧行为一致）
      MAX_FULLTEXT_PER_RUN: "40"
      # 打印阶段耗时日志：1=开, 0=关
      ENABLE_TIMING_LOGS: "1"
      # 并发摘要数量（如需更快/更省配额可调）
      SUMMARIZE_CONCURRENCY: "4"

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PY_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'

      # ✅ 缓存：抓全文/摘要的本地数据库 + NLTK 词库
      - name: Restore aggregator & NLTK cache
        uses: actions/cache@v4
        with:
          path: |
            .cache/aggregator
            summary_cache.json
            ${{ env.NLTK_DATA }}
          key: agg-nlp-${{ runner.os }}-${{ env.PY_VERSION }}-${{ hashFiles('feeds.json') }}
          restore-keys: |
            agg-nlp-${{ runner.os }}-${{ env.PY_VERSION }}-

      - name: Install deps
        run: |
          pip install -r requirements.txt

      - name: Ensure NLTK punkt (cached)
        shell: bash
        run: |
          python - << 'PY'
          import os, nltk
          nltk.data.path.insert(0, os.environ.get("NLTK_DATA",""))
          try:
              nltk.data.find("tokenizers/punkt")
              print("[nltk] punkt found in cache")
          except LookupError:
              print("[nltk] downloading punkt ...")
              nltk.download("punkt", download_dir=os.environ.get("NLTK_DATA",""))
          PY

      - name: Run aggregator
        # 失败不继续生成站点，以免发布空页；但会显示完整日志
        run: |
          set -e
          python scripts/aggregator.py

      - name: Generate sitemap
        run: |
          python scripts/generate_sitemap.py

      # ✅ 只提交产物；缓存由 actions/cache 持久化
      - name: Commit & push (safe retry)
        run: |
          set -e
          git config --global user.name "github-actions"
          git config --global user.email "actions@github.com"
          git add posts/ sitemap.xml
          git commit -m "🤖 Auto-update content and sitemap" || { echo "Nothing to commit"; exit 0; }
          git push origin HEAD:main || {
            echo "Push failed; refetch + rebase then retry..."
            git fetch origin main
            git pull --rebase origin main
            git push origin HEAD:main
          }
